{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import copy\n",
    "import random\n",
    "\n",
    "sys.path.append(\"/home/tejas/projects/teach/src/\")\n",
    "\n",
    "from teach.dataset.definitions import Definitions\n",
    "from teach.dataset.dataset import Dataset\n",
    "from teach.dataset.actions import Action_Keyboard, Action_ObjectInteraction\n",
    "\n",
    "# Edit data directory if changed when using `teach_download`\n",
    "data_dir = \"/tmp/teach-dataset\"\n",
    "data_dir = \"/home/shared/teach/\"\n",
    "\n",
    "definitions = Definitions(version=\"2.0\")\n",
    "\n",
    "images_dir = os.path.join(data_dir, \"images/train/\")\n",
    "game_ids = os.listdir(images_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_interactions_with_dialog_acts(game_id, do_print=False):\n",
    "    f = os.path.join(data_dir, f\"games/train/{game_id}.game.json\")\n",
    "    with open(f) as h:\n",
    "        game_dict = json.load(h)\n",
    "    interactions = game_dict[\"tasks\"][0][\"episodes\"][0][\"interactions\"]\n",
    "    previous_context = \"\"\n",
    "    da_interactions = []\n",
    "    for interaction in interactions:\n",
    "        if \"utterance\" in interaction:\n",
    "            role = definitions.map_agents_id2info[interaction[\"agent_id\"]][\"agent_name\"]\n",
    "            role_actual = \"User\" if role == \"Commander\" else \"Assistant\"\n",
    "            utterance = interaction[\"utterance\"]\n",
    "            if do_print:\n",
    "                print(f\"{role}: {utterance}\")\n",
    "            for idx in range(len(interaction[\"da_metadata\"][\"das\"])):\n",
    "                # interaction[\"da_metadata\"][\"text_segments\"] and interaction[\"da_metadata\"][\"das\"] are lists of length 3\n",
    "                # If an utterance has fewer than 3 DAs then the extra segments and DAs are empty\n",
    "                # No utterance has more than 3 DAs\n",
    "                utt_segment = interaction[\"da_metadata\"][\"text_segments\"][idx]\n",
    "                da = interaction[\"da_metadata\"][\"das\"][idx].strip()\n",
    "                if da == \"\" or previous_context == \"\":\n",
    "                    continue\n",
    "                if role_actual == \"Assistant\":\n",
    "                    da_interactions.append({\n",
    "                        \"context\": previous_context,\n",
    "                        \"response\": f\"{role_actual}: {utt_segment}\",\n",
    "                        \"dialog_act\": da\n",
    "                    })\n",
    "            previous_context += f\"{role_actual}: {utterance} \\n\"\n",
    "    return da_interactions\n",
    "\n",
    "\n",
    "game_id = random.choice(game_ids)\n",
    "get_interactions_with_dialog_acts(game_id, do_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "da2instances = defaultdict(list)\n",
    "for g in tqdm(game_ids):\n",
    "    game_interactions = get_interactions_with_dialog_acts(g)\n",
    "    for interaction in game_interactions:\n",
    "        da2instances[interaction[\"dialog_act\"]].append(interaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_selected_instances = []\n",
    "random.seed(0)\n",
    "for da, instances in da2instances.items():\n",
    "    if len(instances) < 50:\n",
    "        continue\n",
    "    print(da, len(instances))\n",
    "    filtered_instances = random.choices(instances, k=min(50, len(instances)))\n",
    "    all_selected_instances.extend(filtered_instances)\n",
    "\n",
    "print(f\"Total instances: {len(all_selected_instances)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Friction:\n",
    "\n",
    "    with open('system_new.txt', 'r') as txt:\n",
    "        system = '\\n'.join(txt.readlines())\n",
    "\n",
    "    def prompt(x):\n",
    "        p = f'The example dialogue is provided next.\\n\\n{x[\"context\"]}\\n\\n\\n'\n",
    "        try:\n",
    "            p += f'The response is provided below.\\n\\nResponse: {x[\"response\"]}\\n\\n\\n'\n",
    "        except KeyError:\n",
    "            p += f'The response is provided below.\\n\\nResponse: \\n\\n\\n'\n",
    "        p += 'What friction category if any?'\n",
    "        return p\n",
    "\n",
    "f = Friction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from openai_utils import openai_caller\n",
    "\n",
    "idx = random.choice(range(len(all_selected_instances)))\n",
    "d = all_selected_instances[idx]\n",
    "print(f\"Context: {d['context']}\")\n",
    "print(f\"Response: {d['response']}\")\n",
    "print(f\"Dialog acts: {d['dialog_act']}\")\n",
    "\n",
    "prompt = Friction.system + Friction.prompt(d)\n",
    "messages = [{'role': 'system', 'content': prompt}]\n",
    "\n",
    "gpt_response = openai_caller(messages, max_new_tokens=256, model='gpt4o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai, logging\n",
    "\n",
    "logging.getLogger(\"openai\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"httpcore\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"httpx\").setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "openai_caller.reset_tokens_used()\n",
    "annotated_data = []\n",
    "\n",
    "for d in tqdm(all_selected_instances):\n",
    "        prompt = Friction.system + Friction.prompt(d)\n",
    "        messages = [{'role': 'system', 'content': prompt}]\n",
    "        gpt_response = openai_caller(messages, max_new_tokens=256, model='gpt4o')\n",
    "        friction_anno = gpt_response.split('ANSWER = ')[-1]\n",
    "        new_d = d.copy()\n",
    "        new_d['friction_anno'] = friction_anno\n",
    "        new_d['gpt_response'] = gpt_response\n",
    "        annotated_data.append(new_d)\n",
    "print(f\"Annotation cost: ${openai_caller.compute_cost():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "annotations = [x['friction_anno'] for x in annotated_data]\n",
    "print(Counter(annotations))\n",
    "\n",
    "import json\n",
    "json.dump(annotated_data, open(f'data_new_prompt/teach_anno-{len(annotated_data)}instances.json', 'w'), indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acttype_frictionanno_counter = Counter()\n",
    "for d in annotated_data:\n",
    "    d['friction_anno'] = d['friction_anno'].strip('.')\n",
    "    acttype_frictionanno_counter[(d['dialog_act'], d['friction_anno'])] += 1\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "rows = set([x[0] for x in acttype_frictionanno_counter.keys()])\n",
    "cols = set([x[1] for x in acttype_frictionanno_counter.keys()])\n",
    "data = [[acttype_frictionanno_counter[(r, c)]/sum([acttype_frictionanno_counter[(r, c2)] for c2 in cols]) for c in cols] for r in rows]\n",
    "df = pd.DataFrame(data)#, index=rows, columns=cols)\n",
    "plt.figure(figsize=(8, 8), dpi=150)\n",
    "sns.heatmap(df, annot=True, fmt='.2f', cmap='viridis')\n",
    "plt.xlabel('Friction Category', fontsize=14)\n",
    "plt.ylabel('Dialog Act type', fontsize=14)\n",
    "plt.xticks(ticks=[x+0.5 for x in range(len(cols))], labels=cols, rotation=45, fontsize=12)\n",
    "plt.yticks(ticks=[x+0.5 for x in range(len(rows))], labels=rows, rotation=0, fontsize=12)\n",
    "plt.title('Fraction of instances in each TEACh Dialog Act type \\nclassified under different Friction categories', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "annotations = json.load(open('data_new_prompt/teach_anno-550instances.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "das = set([x['dialog_act'] for x in annotations])\n",
    "da2annos = {da: [x for x in annotations if x['dialog_act'] == da] for da in das}\n",
    "fcs = set([x['friction_anno'] for x in annotations])\n",
    "fc2annos = {fc: [x for x in annotations if x['friction_anno'] == fc] for fc in fcs}\n",
    "dafc2annos = {(da, fc): [x for x in annotations if x['dialog_act'] == da and x['friction_anno'] == fc] for da in das for fc in fcs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: what can i do today \n",
      "User: make a slice of tomato \n",
      "User: tomato is on the chair \n",
      "User: knife is on the left side to the oven \n",
      "Assistant: done \n",
      "User: potato is inside the wash basin \n",
      "Assistant: what can i do next \n",
      "User: slice it \n",
      "User: and cook it in the microwave \n",
      "Assistant: am i to make a slice of tomatoe or potatoe? \n",
      "User: both \n",
      "User: tomato slicing done \n",
      "User: now potato \n",
      "User: potato is inside wash basin \n",
      "User: turn off the tap to find it \n",
      "User: left side basin \n",
      "Assistant: i have sliced the potatoe and tomatoe \n",
      "User: cook a slice of potato in the microwave \n",
      "Assistant: cant seem to be able to put the knife down \n",
      "User: put it on the right side of the wash basin \n",
      "User: enough area there \n",
      "Assistant: its still not working \n",
      "Assistant: now its working \n",
      "User: put it on the table \n",
      "User: ok \n",
      "User: remove extra items from the oven \n",
      "User: to place the slice inside directly \n",
      "User: just the slice \n",
      "User: not with plate \n",
      "Assistant: done \n",
      "User: now place the slices in this order on the plate \n",
      "User: tomato potato tomato \n",
      "User: plate beside the oven \n",
      "Assistant: where do i place the tomatoe \n",
      "User: on the plate \n",
      "User: p t p in this order \n",
      "User: sorry \n",
      "User: t p t \n",
      "User: on the plate next to oven \n",
      "User: take thin slice \n",
      "\n",
      "RESPONSE TO ANNOTATE: Assistant: cant seem to be able to place the potatoe\n",
      "\n",
      "ANNOTATION: Reflective Pause\n",
      "GPT REASONING: To determine the friction category for the response \"Assistant: cant seem to be able to place the potatoe,\" we need to analyze the situation. The assistant is expressing difficulty in performing an action, which might indicate an internal reflection or a change in the environment that requires attention. The response doesn't explicitly restate previous information (Reinforcement), reveal assumptions (Assumption Reveal), provide overly specific information (Overspecification), or ask for clarification (Probing). It fits best with the category of Reflective Pause, as the assistant is pausing to indicate a problem or doubt in completing the task.\n",
      "\n",
      "ANSWER = Reflective Pause\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "#x = random.choice(da2annos[\"MiscOther\"])\n",
    "x = random.choice(fc2annos[\"Reflective Pause\"])\n",
    "\n",
    "print(x['context'])\n",
    "\n",
    "print(\"RESPONSE TO ANNOTATE:\", x['response'])\n",
    "\n",
    "print(\"\\nANNOTATION:\", x['friction_anno'])\n",
    "print(\"GPT REASONING:\", x['gpt_response'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.19 ('teach')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9fb16713afc3fdf83049b920037f53cce81bfb494bc99525613622507270313b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
